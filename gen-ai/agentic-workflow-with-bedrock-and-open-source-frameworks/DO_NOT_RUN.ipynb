{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN\n",
    "\n",
    "이 노트북은 참고용 코드 스니펫만 포함하고 있습니다. 워크샵 중에 이 노트북을 실행하지 마세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제어권 반환을 통한 대화형 에이전트 흐름: 실행 시간 동안 추가 입력을 위한 사용자 상호작용\n",
    "\n",
    "때로는 도구를 실행하거나 상위 수준의 작업을 해결하기 위해 추가 입력이 필요할 수 있습니다. 이 경우, 사용자 피드백을 수집하기 위해 제어권을 사용자에게 반환해야 합니다. LangGraph에서는 이를 브레이크포인트와 같은 개념으로 구현할 수 있습니다: 특정 단계에서 그래프 실행을 중지합니다. 이 브레이크포인트에서 사용자 입력을 기다릴 수 있습니다. 사용자로부터 입력을 받으면 그래프 상태에 추가하고 진행할 수 있습니다. 다음에서는 제어권 반환을 통한 사용자 상호작용을 지원하도록 에이전트 어시스턴트를 확장할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 도구: AskHuman\n",
    "\n",
    "사용자를 흐름에 참여시키기 위해서는, 별도의 도구를 만들어야 합니다. 이를 `AskHuman`이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask missing information from the user\"\"\"\n",
    "\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant capable of providing travel recommendations.\"\n",
    "            \" Use the provided tools to look for personalized travel recommendations and information about specific destinations.\"\n",
    "            \" If you dont have enough information then use AskHuman tool to get required information. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \" If you dont have enough information then use AskHuman tool to get required information. \",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "runnable_with_tools = primary_assistant_prompt | llm.bind_tools(tools + [AskHuman])\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    response = runnable_with_tools.invoke(state)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자에게 질문하기 위한 fake 노드도 정의해야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a fake node to ask the human\n",
    "def ask_human(state):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엣지에 대한 조건부 라우팅을 처리할 수 있는 함수도 정의해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "graph_builder.add_edge(START, \"assistant\")\n",
    "graph_builder.add_node(\"assistant\", Assistant(runnable_with_tools))\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # We may ask the human\n",
    "        \"ask_human\": \"ask_human\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"tools\", \"assistant\")\n",
    "# After we get back the human response, we go back to the agent\n",
    "graph_builder.add_edge(\"ask_human\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = MemorySaver()\n",
    "agent_with_hil = graph_builder.compile(\n",
    "    checkpointer=memory, interrupt_before=[\"ask_human\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent_with_hil.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"I want to book a travel destination\")\n",
    "for event in agent_with_hil.stream(\n",
    "    {\"messages\": [input_message]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 사용자 입력을 흐름에 전달하려고 합니다. 따라서 이 스레드의 상태를 사용자의 응답으로 업데이트해야 합니다. AskHuman을 도구 호출로 취급하고 있으므로, 해당 도구 호출의 ID를 포함하여 도구 호출 응답 스타일로 상태를 업데이트해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_id = (\n",
    "    agent_with_hil.get_state(config).values[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "# We now create the tool call with the id and the response we want\n",
    "tool_message = [\n",
    "    {\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": \"I love beaches!\"}\n",
    "]\n",
    "\n",
    "agent_with_hil.update_state(config, {\"messages\": tool_message}, as_node=\"ask_human\")\n",
    "\n",
    "agent_with_hil.get_state(config).next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 사용자 입력을 상태에 주입했습니다. `.next` 함수는 상태 그래프에서 정의한 대로 워크플로우 `실행`의 다음 단계가 어시스턴트가 될 것임을 명확하게 보여줍니다. 이제 그래프 `실행`을 계속할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in agent_with_hil.stream(None, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중단 후 상태 업데이트가 있는 호텔 에이전트\n",
    "\n",
    "다음 코드 스니펫에서는 민감한 도구에 대해 실행을 중단하고 실행을 계속하기 위한 사용자 승인 후에 상태를 업데이트하는 방법을 보여줄 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 커스텀 호텔 에이전트를 만들기 위해 그래프에 모든 노드를 추가하고 컴파일해보겠습니다.\n",
    "\n",
    "이 그래프는 다음을 포함하는 호텔 예약 시스템의 흐름을 정의할 것입니다:\n",
    "\n",
    "1. 요청을 처리하기 위한 메인 호텔 에이전트 노드\n",
    "2. 호텔 예약 검색 및 조회를 실행하기 위한 도구 노드\n",
    "3. 호텔 예약 취소 및 변경을 위한 또 다른 도구 노드\n",
    "   \n",
    "이 그래프는 현재 상태에 기반하여 다음 단계를 결정하기 위해 조건부 엣지를 사용하여 동적이고 반응적인 워크플로우를 가능하게 합니다. 또한 상호작용 간에 상태를 유지하기 위한 메모리 관리도 설정할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Create a new graph workflow\n",
    "hotel_workflow = StateGraph(MessagesState)\n",
    "\n",
    "hotel_workflow.add_node(\"hotel_agent\", hotel_agent)\n",
    "hotel_workflow.add_node(\"search_and_retrieve_node\", search_and_retrieve_node)\n",
    "hotel_workflow.add_node(\"change_and_cancel_node\", change_and_cancel_node)\n",
    "\n",
    "hotel_workflow.add_edge(START, \"hotel_agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "hotel_workflow.add_conditional_edges(\n",
    "    \"hotel_agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    {\n",
    "        # If agent decides to use `suggest_hotels` or  `retrieve_hotel_booking`\n",
    "        \"continue\": \"search_and_retrieve_node\",\n",
    "        # If agent decides to use `change_hotel_booking` or  `cancel_hotel_booking`\n",
    "        \"human_approval\": \"change_and_cancel_node\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "hotel_workflow.add_edge(\"search_and_retrieve_node\", \"hotel_agent\")\n",
    "hotel_workflow.add_edge(\"change_and_cancel_node\", \"hotel_agent\")\n",
    "\n",
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "hotel_graph_compiled = hotel_workflow.compile(\n",
    "    checkpointer=memory, interrupt_before=[\"change_and_cancel_node\"]\n",
    ")\n",
    "\n",
    "display(Image(hotel_graph_compiled.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_event(event: tuple, _printed: set, max_length=1500):\n",
    "    event_dict = event[1] if isinstance(event, tuple) else event\n",
    "\n",
    "    # Handle dialog state\n",
    "    current_state = event_dict.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "\n",
    "    message = event_dict.get(\"messages\")\n",
    "    if message:\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "\n",
    "        # Get the last message\n",
    "        last_message = message[-1]\n",
    "\n",
    "        message_id = getattr(last_message, \"id\", str(id(last_message)))\n",
    "\n",
    "        if message_id not in _printed:\n",
    "            # Handle pretty printing based on message type\n",
    "            if hasattr(last_message, \"pretty_repr\"):\n",
    "                msg_repr = last_message.pretty_repr(html=True)\n",
    "            else:\n",
    "                msg_repr = f\"Content: {last_message.content}\"\n",
    "                if hasattr(last_message, \"additional_kwargs\"):\n",
    "                    msg_repr += f\"\\nAdditional info: {last_message.additional_kwargs}\"\n",
    "\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "\n",
    "            print(msg_repr)\n",
    "            _printed.add(message_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "_printed = set()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "events = hotel_graph_compiled.stream(\n",
    "    {\"messages\": (\"user\", \"Get details of my booking id 203\")},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "_printed = set()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "events = hotel_graph_compiled.stream(\n",
    "    {\"messages\": (\"user\", \"cancel my hotel booking id 206\")},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\n",
    "    \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "    \" otherwise, explain your requested changed.\\n\\n\"\n",
    ")\n",
    "if user_input.strip() == \"y\":\n",
    "    # Just continue\n",
    "    result = hotel_graph_compiled.invoke(\n",
    "        None,\n",
    "        config,\n",
    "    )\n",
    "    result[\"messages\"][-1].pretty_print()\n",
    "else:\n",
    "    result = hotel_graph_compiled.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                    content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "    result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서브그래프의 상태 업데이트\n",
    "\n",
    "전체 그래프를 컴파일하면 호텔 에이전트가 서브그래프로 추가됩니다. 중단 후에 서브그래프의 상태를 업데이트해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그래프를 테스트할 준비가 되었습니다. 메모리를 관리하기 위해 고유한 thread_id를 생성할 것입니다. 그래프를 테스트하기 위한 몇 가지 샘플 질문이 있습니다.\n",
    "\n",
    "snapshot은 사용자 입력이 필요한 보류 중인 작업이나 결정이 있는지 확인하는 데 필요한 수퍼바이저 에이전트 그래프의 현재 상태를 검색합니다. while 루프 조건에서 snapshot.next 필드를 확인하여 사용자 승인이 필요한 보류 중인 작업이 있는지 확인합니다.\n",
    "\n",
    "사용자 입력을 받은 후 상태를 업데이트합니다. 서브그래프의 상태를 업데이트할 때는 서브그래프의 config인 `state.tasks[0].state.config`를 전달해야 합니다.\n",
    "\n",
    "사용자가 작업을 승인하면 상태를 업데이트하고 그래프를 호출합니다: `supervisor_agent_graph.invoke(None, config, subgraphs=True)`\n",
    "\n",
    "사용자가 작업을 거부하면 도구 메시지로 상태를 업데이트한 다음 그래프를 계속 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_id(pregel_task):\n",
    "    # Navigate to the messages in the state\n",
    "    messages = pregel_task.state.values.get(\"messages\", [])\n",
    "\n",
    "    # Find the last AIMessage\n",
    "    for message in reversed(messages):\n",
    "        if isinstance(message, AIMessage):\n",
    "            # Check if the message has tool_calls\n",
    "            tool_calls = getattr(message, \"tool_calls\", None)\n",
    "            if tool_calls:\n",
    "                # Return the id of the first tool call\n",
    "                return tool_calls[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = str(uuid.uuid4())\n",
    "_printed = set()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "questions = [\n",
    "    \"Get details of my flight booking id 200\",\n",
    "    \"cancel my hotel booking id 136\",\n",
    "]\n",
    "for question in questions:\n",
    "    events = supervisor_agent_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\", subgraphs=True\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = supervisor_agent_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "                \" otherwise, explain your requested changed.\\n\\n\"\n",
    "            )\n",
    "        except:\n",
    "            user_input = \"y\"\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "\n",
    "            supervisor_agent_graph.update_state(\n",
    "                state.tasks[0].state.config,\n",
    "                {\"messages\": \"Yes, cancel my booking\"},\n",
    "                as_node=\"change_and_cancel_node\",\n",
    "            )\n",
    "            result = supervisor_agent_graph.invoke(None, config, subgraphs=True)\n",
    "            result_dict = result[1]\n",
    "            result_dict[\"messages\"][-1].pretty_print()\n",
    "        else:\n",
    "            state = supervisor_agent_graph.get_state(config, subgraphs=True)\n",
    "            tool_id = extract_tool_id(state.tasks[0])\n",
    "            tool_message = [\n",
    "                {\n",
    "                    \"tool_call_id\": tool_id,\n",
    "                    \"type\": \"tool\",\n",
    "                    \"content\": f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                }\n",
    "            ]\n",
    "            supervisor_agent_graph.update_state(\n",
    "                state.tasks[0].state.config,\n",
    "                {\"messages\": tool_message},\n",
    "                as_node=\"change_and_cancel_node\",\n",
    "            )\n",
    "            result = supervisor_agent_graph.invoke(None, config, subgraphs=True)\n",
    "            _print_event(result, _printed)\n",
    "\n",
    "        snapshot = supervisor_agent_graph.get_state(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
