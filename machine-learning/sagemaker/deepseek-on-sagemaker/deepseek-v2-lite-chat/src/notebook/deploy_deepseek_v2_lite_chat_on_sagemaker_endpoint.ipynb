{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd4c755-a6df-424a-9834-00c193d9dbbb",
   "metadata": {},
   "source": [
    "# Deploying the DeepSeek V2 Lite Chat model for text-generation tasks hosted on Amazon SageMaker with DJLServing DLC\n",
    "\n",
    "\n",
    "❗This notebook works well on `ml.t3.medium` instance with `PyTorch 2.2.0 Python 3.10 CPU optimized` kernel from **SageMaker Studio Classic** or `Python3` kernel from **JupyterLab**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989a5c1-070a-4ac3-b944-085349e904b4",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a85b8-c0cb-46bc-b9dc-9f744db4de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install -U pip\n",
    "!pip install -U \"sagemaker>=2.237.3\"\n",
    "!pip install -U \"transformers>=4.47.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d295f44-fcb8-40fa-b281-f7f817dd971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "aws_region, bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a238127-c36e-47dc-a760-f3e6a4f242dc",
   "metadata": {},
   "source": [
    "## Upload the `model.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b872d0-94d7-4a27-9f1f-aa84f1b8f779",
   "metadata": {},
   "source": [
    "Create and upload `model.tar.gz` to our sagemaker session bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e56e5-4c26-4356-9107-dc6e28ba98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p model\n",
    "rm -rf model/*\n",
    "\n",
    "# copy the custom inference script into the working directory\n",
    "cp -rp ../python/code/* model/\n",
    "\n",
    "rm -f model.tar.gz\n",
    "tar --exclude \"*/.ipynb_checkpoints*\" -czvf model.tar.gz model/\n",
    "tar -tvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9f06d-eeb6-4a42-9f97-ec71a3f3c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-ai/DeepSeek-V2-Lite-Chat'\n",
    "\n",
    "base_name = model_name.split('/')[-1].replace('.', '-').lower()\n",
    "base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd6314-488e-41f0-9988-9a07ee5b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(\n",
    "    local_path=\"./model.tar.gz\",\n",
    "    desired_s3_uri=f\"s3://{bucket}/{base_name}\"\n",
    ")\n",
    "\n",
    "print(f\"S3 Code or Model tar ball uploaded to:\\n{s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c8ff8-e758-41cd-8732-110c9f828652",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {s3_model_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77338b4-f313-4074-9728-5a4060becaf3",
   "metadata": {},
   "source": [
    "## Deploy `DeepSeek-V2-Lite-Chat` model to SageMaker Real-time Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2400ed4-5503-4b20-b351-ac2d50ea0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-lmi\",\n",
    "    region=aws_region,\n",
    "    version=\"0.30.0\",\n",
    "    py_version=\"py311\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cabdce-8762-4c0e-a934-3b14532cdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sm_model_name = name_from_base(base_name, short=True).lower()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "model = Model(\n",
    "    name=sm_model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=s3_model_uri,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaf92a-96a9-428b-be5a-75a59a903854",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True).lower()\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "_predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ca705-0638-4b9a-aa10-b454b1af976d",
   "metadata": {},
   "source": [
    "# Create a Predictor with SageMaker Endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3ccb7-e069-49f3-a51b-bc458ae3fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99574d3f-1dfb-4773-b08a-7b14797cd7a3",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c90f16-728b-47fd-90a6-38767a50aafc",
   "metadata": {},
   "source": [
    "### Standard schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e90088-07ee-45c7-96eb-be8250d055b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68b3ce-1bca-4895-9db0-97dfc52b0a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This code works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Help me write a quick sort code\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": inputs, \"parameters\": parameters}\n",
    ")\n",
    "\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908dbff-b140-4582-bc4a-9b168cce5799",
   "metadata": {},
   "source": [
    "### Message API\n",
    "\n",
    "- Ref: https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html#message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d51f2-b237-4fa0-b3c8-437b14bf8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-139991376379568\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1736684437,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \" Sure, here's a simple implementation of the Quick Sort algorithm in Python:\\n\\n```python\\ndef quicksort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[len(arr) // 2]\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quicksort(left) + middle + quicksort(right)\\n\\n\\nprint(quicksort([3,6,8,10,1,2,1]))\\n```\\n\\nIn this code:\\n\\n- We first handle the base case where the input list is 1 item or less. \\n- Then we choose a pivot element (usually the middle one). \\n- We create three lists: `left` for elements less than the pivot, `middle` for elements equal to the pivot, and `right` for elements greater than the pivot. \\n- We recursively sort the `left` and `right` sublists and combine them with the `middle` list. \\n- The function returns the sorted list of elements.\\n\\nThis code sorts the array in-place (i.e., it does not create a new list, but modifies the existing one). If you need a new list without modifying the original one, you can add `return arr[:]`.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"eos_token\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"completion_tokens\": 312,\n",
      "    \"total_tokens\": 329\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "prompt = \"Help me write a quick sort code in python\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = predictor.predict({\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 500\n",
    "})\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2d6fa-1a86-4e00-8555-0adaf0ae9068",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92399974-8c06-47b4-9b68-6958bda9b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "In more detail, the attention function can be described as follows:\n",
      "\n",
      "1. Given a query `q` and a set of keys `K = {k1, k2, ..., kN}` and values `V = {v1, v2, ..., vN}`, the attention function computes a set of attention coefficients `α = {α1, α2, ..., αN}`.\n",
      "\n",
      "2. Each attention coefficient `αi` is computed as a function of the query `q` and the key `ki`, typically using a compatibility function such as the dot product or the additive angular margin (AAM) function.\n",
      "\n",
      "3. The output `o` is computed as a weighted sum of the values `V` using the attention coefficients `α`:\n",
      "\n",
      "   `o = Σ(αi * vi)`\n",
      "\n",
      "4. The attention coefficients `αi` are normalized to sum up to 1, typically using a softmax function:\n",
      "\n",
      "   `αi = exp(qi * ki) / Σj exp(qi * kj)`\n",
      "\n",
      "In practice, the attention function is used in various deep learning models, such as the Transformer architecture, which is widely used for tasks like machine translation, text summarization, and question answering.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 500,\n",
    "}\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": inputs, \"parameters\": parameters}\n",
    ")\n",
    "\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4f822-07ea-4edf-9ee0-9c524f57a4f5",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af61448-3662-4837-917e-c617f45c5373",
   "metadata": {},
   "source": [
    "### Standard schema streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fafca-4be6-4b6d-9f8b-7c1c46cf1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from sagemaker.iterators import BaseIterator\n",
    "\n",
    "\n",
    "class TokenIterator(BaseIterator):\n",
    "    def __init__(self, stream):\n",
    "        super().__init__(stream)\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                full_line = line[:-1].decode('utf-8')\n",
    "                line_data = json.loads(full_line.lstrip(\"data:\").rstrip(\"\\n\"))\n",
    "                return line_data[\"token\"].get(\"text\", \"\")\n",
    "            chunk = next(self.byte_iterator)\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cd20c-805c-4e5f-b606-bfe6fa4a0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Help me write a quick sort code\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": inputs,\n",
    "    \"parameters\": parameters,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response_stream = predictor.predict_stream(\n",
    "    data=payload,\n",
    "    custom_attributes=\"accept_eula=false\",\n",
    "    iterator=TokenIterator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516fed4-fe33-4a2b-9008-1233066048e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here's a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quick_sort([3,6,8,10,1,2,1]))\n",
      "```\n",
      "\n",
      "This code works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted."
     ]
    }
   ],
   "source": [
    "for token in response_stream:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e90c07-3533-47b9-ba63-bfef63179a83",
   "metadata": {},
   "source": [
    "### Message Schema streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e4204-a815-4645-93f6-699fd1da153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from sagemaker.iterators import BaseIterator\n",
    "\n",
    "\n",
    "class MessageTokenIterator(BaseIterator):\n",
    "    def __init__(self, stream):\n",
    "        super().__init__(stream)\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                full_line = line[:-1].decode('utf-8')\n",
    "                line_data = json.loads(full_line.lstrip('data:').rstrip('\\n'))\n",
    "                return line_data['choices'][0]['delta'].get('content', '')\n",
    "            chunk = next(self.byte_iterator)\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ae9a0-f624-4580-a10f-ac2d7f043d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Help me write a quick sort code\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"stream\": \"true\"\n",
    "}\n",
    "\n",
    "response_stream = predictor.predict_stream(\n",
    "    data=payload,\n",
    "    custom_attributes=\"accept_eula=false\",\n",
    "    iterator=MessageTokenIterator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21109b95-b22a-48c3-9195-58340d7d8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in response_stream:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3712ee2-0a8c-42f1-9cad-560bd09785cd",
   "metadata": {},
   "source": [
    "# Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17685ee9-8991-4bb7-a585-3d0c8cf174b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28168876-e166-4572-a482-d60b8a78d0d8",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [DeepSeek V2 Lite Chat Model Card](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat)\n",
    "- [deepseek-ai/deepseek-coder-6.7b-instruct SageMaker LMI deployment guide](https://github.com/aws-samples/llm_deploy_gcr/blob/main/sagemaker/deepseek_coder_6.7_instruct.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
