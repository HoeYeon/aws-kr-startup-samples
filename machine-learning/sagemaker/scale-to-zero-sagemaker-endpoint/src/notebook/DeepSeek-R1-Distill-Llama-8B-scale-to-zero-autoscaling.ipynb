{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b55126-c364-435f-a3d8-5a992dd854e0",
   "metadata": {},
   "source": [
    "# Reducing Inference Costs on DeepSeek-R1-Distill-Llama-8B with SageMaker Inference's Scale to Zero Capability\n",
    "\n",
    "This demo notebook demonstrate how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance.\n",
    "\n",
    "❗This notebook works well on `ml.t3.medium` instance with `PyTorch 2.2.0 Python 3.10 CPU optimized` kernel from **SageMaker Studio Classic** or `Python3` kernel from **JupyterLab**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cdfef-0c42-4f09-af67-dc2b4499a75a",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce148393-c6e4-4018-870e-9d8f0192c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d1c4a-306b-450c-8996-297f2fd1a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_cfn_outputs(stackname: str, region_name: str='us-east-1') -> List:\n",
    "    cfn = boto3.client('cloudformation', region_name=region_name)\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa4d34-f90f-4947-97ba-4a60bd63915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFN_STACK_NAME = \"SageMakerInferenceComponent\" # name of CloudFormation stack\n",
    "\n",
    "cfn_outputs = get_cfn_outputs(CFN_STACK_NAME, region_name=boto_region)\n",
    "endpoint_name = cfn_outputs['EndpointName']\n",
    "inference_component_name = cfn_outputs['InferenceComponentName']\n",
    "\n",
    "endpoint_name, inference_component_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24f7d8-b5b5-4215-b4d4-c8d560282802",
   "metadata": {},
   "source": [
    "## Create a Predictor with SageMaker Endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6241a5-2f32-4596-84ea-3a03d6c16f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063422-0b95-4917-b29c-4999f4b80ed7",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ac636-a182-487c-a64b-8d68cb7cc498",
   "metadata": {},
   "source": [
    "SageMaker python sdk simplifies the inference construct using `sagemaker.Predictor` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f35bff-ec63-4880-86f5-bd954c321532",
   "metadata": {},
   "source": [
    "`DeepSeek Llama8b` variant is based on 3.1 Llama8b prompt format which is as shown below,\n",
    "\n",
    "```json\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2024\n",
    "Today Date: 29 Jan 2025\n",
    "\n",
    "You are a helpful assistant that thinks and reasons before answering.\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "How many R are in STRAWBERRY? Keep your answer and explanation short!\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0c8f6-f3b1-44ff-af2f-80315061ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "\n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and\n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "\n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding an inferred prefix\n",
    "    system_prefix = f\"\\n\\nCutting Knowledge Date: December 2024\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if entry['role'] == 'system':\n",
    "            output += f\"{system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif entry['role'] != 'system' and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "def send_prompt(predictor, initial_args, messages, parameters):\n",
    "    # convert u/a format\n",
    "    frmt_input = format_messages(messages)\n",
    "    payload = {\n",
    "        \"inputs\": frmt_input,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response = predictor.predict(\n",
    "        initial_args=initial_args,\n",
    "        data=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49d906-3182-43a9-ae68-f9e2b0a8ba74",
   "metadata": {},
   "source": [
    "### Test the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bf32f-9e0f-4279-b9a8-3b2d66a39be9",
   "metadata": {},
   "source": [
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702725-39a6-4c44-9c08-158261ab2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57524f90-685d-48a2-bd76-15106ffe1c3c",
   "metadata": {},
   "source": [
    "## Automatically Scale To Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed24935-167d-4078-ac97-8a308d3c0e95",
   "metadata": {},
   "source": [
    "### Scaling policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66f0c1-cf77-4f55-a617-f1ef8b8d0a20",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "- A [target tracking policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) that can scale in the copy count for our inference component model copies to zero, and from 1 to n.\n",
    "- A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862d871-8fde-465c-bc58-9cb31da6cf4f",
   "metadata": {},
   "source": [
    "### Testing the behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e8303-1a49-4dd1-8e4e-4d0a21f6014c",
   "metadata": {},
   "source": [
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a7e7e-b020-4d32-b2ea-99ac00a68c9c",
   "metadata": {},
   "source": [
    "### Inference Component (IC) copy count scales in to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5b592-26b5-41da-a224-bba3ca9bb271",
   "metadata": {},
   "source": [
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070957a-88ce-421d-8e36-2eaac773805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=boto_region)\n",
    "\n",
    "time.sleep(600)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d63e34-739a-4753-8eed-bcd1fcfb3b86",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28df49c-6d3f-471b-8968-2b0624d5252a",
   "metadata": {},
   "source": [
    "After a few additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878338-6eb3-41c4-b8de-4d09e414f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 mins instances will scale down to 0\n",
    "time.sleep(60)\n",
    "\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e76db-cff8-49f8-9df8-e577b7efe9d5",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7643bc1-dc7d-48ad-9a36-1d312f918520",
   "metadata": {},
   "source": [
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870552eb-7c17-46bf-add7-35efd0d456cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e93d3-acd7-477f-981f-f0ce27c676ef",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66972ce8-f497-4813-944f-9537c326d2d3",
   "metadata": {},
   "source": [
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c686c-bdb1-47d6-a4d8-464f2327a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 min instances will scale out from zero to one\n",
    "time.sleep(60)\n",
    "\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a894f-b022-4243-9dbf-e01405a6147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214201b-fedd-4112-8068-a61859ca37ce",
   "metadata": {},
   "source": [
    "### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64296545-6396-4452-a037-1824f9760ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db745067-8166-4f83-8af8-e0aa1f651eb0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [✍🏻 (AWS Machine Learning Blog) Unlock cost savings with the new scale down to zero feature in SageMaker Inference (2024-12-02)](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)\n",
    "- [💻 Unlock Cost Savings with New Scale-to-Zero Feature in SageMaker Inference](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/scale-to-zero-endpoint/llama3-8b-scale-to-zero-autoscaling.ipynb)\n",
    "- [💻 Deploy DeepSeek R1 Large Language Model from HuggingFace Hub on Amazon SageMaker](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Deepseek/DeepSeek-R1-Llama8B-LMI-TGI-Deploy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
