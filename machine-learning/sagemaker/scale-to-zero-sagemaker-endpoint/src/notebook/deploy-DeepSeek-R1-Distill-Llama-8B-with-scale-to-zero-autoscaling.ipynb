{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b55126-c364-435f-a3d8-5a992dd854e0",
   "metadata": {},
   "source": [
    "# Reducing Inference Costs on DeepSeek-R1-Distill-Llama-8B with SageMaker Inference's Scale to Zero Capability\n",
    "\n",
    "This demo notebook demonstrate how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance.\n",
    "\n",
    "❗This notebook works well on `ml.t3.medium` instance with `PyTorch 2.2.0 Python 3.10 CPU optimized` kernel from **SageMaker Studio Classic** or `Python3` kernel from **JupyterLab**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cdfef-0c42-4f09-af67-dc2b4499a75a",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be013614-d1f4-4e16-afdf-f255da02e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "\n",
    "!pip install -U pip\n",
    "!pip install -U \"sagemaker>=2.239.0\"\n",
    "!pip install -U \"transformers>=4.47.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce148393-c6e4-4018-870e-9d8f0192c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efc81e-7165-4195-a8b9-e82f4690db95",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49503c-c79f-40fa-a771-465f436824a6",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5fed67-3f53-4b3e-9c3e-4ecfec6ce87f",
   "metadata": {},
   "source": [
    "#### Deploy using DJL-Inference Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037141b-0acd-4c1b-8ca3-93d87efce56f",
   "metadata": {},
   "source": [
    "The [Deep Java Library (DJL) Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) containers are specialized Docker containers designed to facilitate the deployment of large language models (LLMs) on Amazon SageMaker. These containers integrate a model server with optimized inference libraries, providing a comprehensive solution for serving LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33f008-f52b-4dd7-a1ad-c1e7077f2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can get inference image uri programmatically using sagemaker.image_uris.retrieve\n",
    "# deepspeed_image_uri = sagemaker.image_uris.retrieve(\n",
    "#     framework=\"djl-inference\",\n",
    "#     region=boto_region,\n",
    "#     version=\"0.31.0-lmi13.0.0-cu124\"\n",
    "# )\n",
    "\n",
    "djllmi_inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70431d26-8087-47c4-b1ea-43cb1614ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(\"deepseek-r1-distill-llama3-8b\", short=True)\n",
    "\n",
    "deepseek_lmi_model = sagemaker.Model(\n",
    "    image_uri=djllmi_inference_image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"10000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "    },\n",
    "    role=role,\n",
    "    name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab16b2-b10b-48d7-82df-8b2b2d3eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "resources_required = ResourceRequirements(\n",
    "    requests={\n",
    "        \"num_cpus\": 2,\n",
    "        \"memory\": 1024,\n",
    "        \"num_accelerators\": 1,\n",
    "        \"copies\": 1, # specify the number of initial copies (default is 1)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f5604-da2f-43f1-b8e3-65fdbffeb33e",
   "metadata": {},
   "source": [
    "We begin by creating an endpoint with setting **MinInstanceCount** to **0**. This allows the endpoint to scale in all the way down to zero instances when not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a50603-6140-445b-a49e-e65156424a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.enums import EndpointType\n",
    "\n",
    "endpoint_name = name_from_base(\"deepseek-r1-distill-llama3-8b-scale-to-zero-aas-ep\", short=True)\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "deepseek_lmi_model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    accept_eula=True,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=model_data_download_timeout_in_seconds,\n",
    "    container_startup_health_check_timeout=container_startup_health_check_timeout_in_seconds,\n",
    "    resources=resources_required,\n",
    "    managed_instance_scaling={\n",
    "        \"Status\": \"ENABLED\",\n",
    "        \"MinInstanceCount\": min_instance_count,\n",
    "        \"MaxInstanceCount\": max_instance_count,\n",
    "    },\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    routing_config={\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    ")\n",
    "\n",
    "print(f\"Your DJL-LMI Model Endpoint: {endpoint_name} is now deployed! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24f7d8-b5b5-4215-b4d4-c8d560282802",
   "metadata": {},
   "source": [
    "### Create a Predictor with SageMaker Endpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6241a5-2f32-4596-84ea-3a03d6c16f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb582b-7395-4a24-9dc9-f8e75cf2f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client(\"sagemaker\", region_name=boto_region)\n",
    "\n",
    "response = sagemaker_client.list_inference_components(EndpointNameEquals=predictor.endpoint_name)\n",
    "inference_component_name = response['InferenceComponents'][0]['InferenceComponentName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063422-0b95-4917-b29c-4999f4b80ed7",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ac636-a182-487c-a64b-8d68cb7cc498",
   "metadata": {},
   "source": [
    "SageMaker python sdk simplifies the inference construct using `sagemaker.Predictor` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f35bff-ec63-4880-86f5-bd954c321532",
   "metadata": {},
   "source": [
    "`DeepSeek Llama8b` variant is based on 3.1 Llama8b prompt format which is as shown below,\n",
    "\n",
    "```json\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2024\n",
    "Today Date: 29 Jan 2025\n",
    "\n",
    "You are a helpful assistant that thinks and reasons before answering.\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "How many R are in STRAWBERRY? Keep your answer and explanation short!\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0c8f6-f3b1-44ff-af2f-80315061ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "\n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and\n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "\n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding an inferred prefix\n",
    "    system_prefix = f\"\\n\\nCutting Knowledge Date: December 2024\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if entry['role'] == 'system':\n",
    "            output += f\"{system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif entry['role'] != 'system' and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "def send_prompt(predictor, initial_args, messages, parameters):\n",
    "    # convert u/a format\n",
    "    frmt_input = format_messages(messages)\n",
    "    payload = {\n",
    "        \"inputs\": frmt_input,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response = predictor.predict(\n",
    "        initial_args=initial_args,\n",
    "        data=payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49d906-3182-43a9-ae68-f9e2b0a8ba74",
   "metadata": {},
   "source": [
    "### Test the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bf32f-9e0f-4279-b9a8-3b2d66a39be9",
   "metadata": {},
   "source": [
    "Now we can invoke our endpoint with sample text to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702725-39a6-4c44-9c08-158261ab2905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out how many times the letter R appears in the word \"STRAWBERRY.\" Let me start by writing out the word and looking at each letter one by one. S, T, R, A, W, B, E, R, R, Y. Hmm, I see an R right there in the third position. Then, after that, I see two more Rs at the end: R and R. So that's three Rs in total. Wait, let me count again to make sure I didn't miss any. S, T, R, A, W, B, E, R, R, Y. Yep, that's three Rs. I don't think I missed any other letters. So the answer should be three Rs.\n",
      "</think>\n",
      "\n",
      "The letter R appears three times in STRAWBERRY.\n",
      "\n",
      "Step-by-step explanation:\n",
      "1. Write out the word: S, T, R, A, W, B, E, R, R, Y.\n",
      "2. Identify each R: The third letter is R, and the eighth and ninth letters are also R.\n",
      "3. Count the Rs: There are three Rs in total.\n",
      "\n",
      "Answer: 3\n",
      "CPU times: user 12.7 ms, sys: 3.78 ms, total: 16.4 ms\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57524f90-685d-48a2-bd76-15106ffe1c3c",
   "metadata": {},
   "source": [
    "## Automatically Scale To Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed24935-167d-4078-ac97-8a308d3c0e95",
   "metadata": {},
   "source": [
    "### Scaling policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66f0c1-cf77-4f55-a617-f1ef8b8d0a20",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "- A [target tracking policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) that can scale in the copy count for our inference component model copies to zero, and from 1 to n.\n",
    "- A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df98be9-c9d0-4b8c-84f1-17803ff27a81",
   "metadata": {},
   "source": [
    "### Scaling policy for inference components copies (target tracking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa718f6a-3e1c-46a6-834e-fd22a63a4439",
   "metadata": {},
   "source": [
    "We start with creating our target tracking policies for scaling the CopyCount of our inference component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b4bdf-8cac-44ad-a27b-f4838b170791",
   "metadata": {},
   "source": [
    "#### Register a new autoscaling target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668075ac-217c-436f-ad40-976d182132b4",
   "metadata": {},
   "source": [
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling.\n",
    "In the following code block, you set **MinCapacity** to **0**, which is required for your endpoint to scale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762360b9-fca7-437e-8908-ffdb4a1b40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\", region_name=boto_region)\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\", region_name=boto_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8cdd3-aac5-4960-959a-cb444325be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = min_instance_count\n",
    "max_copy_count = max_instance_count\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfc9b9-ffda-4cfd-9fcc-454cd4ae93cf",
   "metadata": {},
   "source": [
    "#### Configure Target Tracking Scaling Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0144c-449d-4b9f-982c-4d96250ebc0c",
   "metadata": {},
   "source": [
    "Once you have registered your new scalable target, the next step is to define your target tracking policy.\n",
    "In the code example that follows, we set the TargetValue to 5.\n",
    "This setting instructs the auto-scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5.\n",
    "Here we are taking advantage of the more granular auto scaling metric `PredefinedMetricType`: `SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution` to more accurately monitor and react to changes in inference traffic. Take a look this [blog](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13ad0b-1797-4f47-bbb8-c73acf8ec48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-deepseek-r1-distill-llama3-8b-scale-to-zero-aas-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168a1d2-2e2f-4481-af72-23a061bcaeae",
   "metadata": {},
   "source": [
    "Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1–2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7384f8-f47a-4ddd-be0c-bef6b0b7f373",
   "metadata": {},
   "source": [
    "### Scale out from zero policy (step scaling policy )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6840f-96db-4e8f-b7f9-7f6aef2f967e",
   "metadata": {},
   "source": [
    "To enable your endpoint to scale out from zero instances, do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882f92a-0dbb-410a-8c4d-ecf2af37e2e5",
   "metadata": {},
   "source": [
    "#### Configure Step Scaling Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a54ba9-11ba-49bd-9928-118b5964f1a1",
   "metadata": {},
   "source": [
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle. The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ddf9d-0535-4af6-9dc6-1d608f892ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy name for the step scaling policy\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-{inference_component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90006406-3b2e-4a78-9a29-c52225ae2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb426c42-b344-411f-bc70-d01f41716845",
   "metadata": {},
   "source": [
    "#### Create the CloudWatch alarm that will trigger our policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3cf45-604f-463e-b8a1-8a88cfd74502",
   "metadata": {},
   "source": [
    "Finally, create a CloudWatch alarm with the metric **NoCapacityInvocationFailures**. When triggered, the alarm initiates the previously defined scaling policy. For more information about the **NoCapacityInvocationFailures** metric, see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-inference-component).\n",
    "\n",
    "We have also set the following:\n",
    "\n",
    "- EvaluationPeriods to 1\n",
    "- DatapointsToAlarm to 1\n",
    "- ComparisonOperator to GreaterThanOrEqualToThreshold\n",
    " \n",
    "This results in 1 min waiting for the step scaling policy to trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65748e7-d175-474b-a524-2888127fbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-{inference_component_name}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': inference_component_name  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # Set a lower period\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862d871-8fde-465c-bc58-9cb31da6cf4f",
   "metadata": {},
   "source": [
    "### Testing the behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e8303-1a49-4dd1-8e4e-4d0a21f6014c",
   "metadata": {},
   "source": [
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a7e7e-b020-4d32-b2ea-99ac00a68c9c",
   "metadata": {},
   "source": [
    "### Inference Component (IC) copy count scales in to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5b592-26b5-41da-a224-bba3ca9bb271",
   "metadata": {},
   "source": [
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070957a-88ce-421d-8e36-2eaac773805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "time.sleep(600)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d63e34-739a-4753-8eed-bcd1fcfb3b86",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28df49c-6d3f-471b-8968-2b0624d5252a",
   "metadata": {},
   "source": [
    "After a few additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878338-6eb3-41c4-b8de-4d09e414f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 mins instances will scale down to 0\n",
    "time.sleep(60)\n",
    "\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e76db-cff8-49f8-9df8-e577b7efe9d5",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7643bc1-dc7d-48ad-9a36-1d312f918520",
   "metadata": {},
   "source": [
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870552eb-7c17-46bf-add7-35efd0d456cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e93d3-acd7-477f-981f-f0ce27c676ef",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66972ce8-f497-4813-944f-9537c326d2d3",
   "metadata": {},
   "source": [
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c686c-bdb1-47d6-a4d8-464f2327a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 min instances will scale out from zero to one\n",
    "time.sleep(60)\n",
    "\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a894f-b022-4243-9dbf-e01405a6147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214201b-fedd-4112-8068-a61859ca37ce",
   "metadata": {},
   "source": [
    "### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64296545-6396-4452-a037-1824f9760ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out how many times the letter R appears in the word \"STRAWBERRY.\" Let me start by writing out the word and looking at each letter one by one. S, T, R, A, W, B, E, R, R, Y. Hmm, I see an R right there in the third position. Then, after that, I see two more Rs at the end: R and R. So that's three Rs in total. Wait, let me count again to make sure I didn't miss any. S, T, R, A, W, B, E, R, R, Y. Yep, that's three Rs. I don't think I missed any other letters. So the answer should be three Rs.\n",
      "</think>\n",
      "\n",
      "The letter R appears three times in STRAWBERRY.\n",
      "\n",
      "Step-by-step explanation:\n",
      "1. Write out the word: S, T, R, A, W, B, E, R, R, Y.\n",
      "2. Identify each R: The third letter is R, and the eighth and ninth letters are also R.\n",
      "3. Count the Rs: There are three Rs in total.\n",
      "\n",
      "Answer: 3\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_prompt(\n",
    "    predictor=predictor,\n",
    "    initial_args={\n",
    "        'InferenceComponentName': inference_component_name\n",
    "    },\n",
    "    messages=messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0c056-23d8-4cee-a91a-f001175f6d33",
   "metadata": {},
   "source": [
    "### Optionally clean up the environment\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c1318c-529b-4c68-8628-a57ab74d0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    aas_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=scalable_dimension,\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "try:\n",
    "    cloudwatch_client.delete_alarms(AlarmNames=[step_scaling_alarm_name])\n",
    "    print(f\"Deleted CloudWatch step scaling scale-out alarm [b]{step_scaling_alarm_name} ✅\")\n",
    "except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"CloudWatch scale-out alarm [b]{step_scaling_alarm_name}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete step scaling policies\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    aas_client.delete_scaling_policy(\n",
    "        PolicyName=step_scaling_policy_name,\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Deleted scaling policy [i green]{step_scaling_policy_name} ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scaling policy [i]{step_scaling_policy_name}[/i] not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822bdae-ebc7-49d4-96ff-0158b67c4b1b",
   "metadata": {},
   "source": [
    "- Delete inference component\n",
    "- Delete endpoint\n",
    "- delete endpoint-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b5ffe-9597-43a7-8595-b20c684cbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db745067-8166-4f83-8af8-e0aa1f651eb0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [✍🏻 (AWS Machine Learning Blog) Unlock cost savings with the new scale down to zero feature in SageMaker Inference (2024-12-02)](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)\n",
    "- [💻 Unlock Cost Savings with New Scale-to-Zero Feature in SageMaker Inference](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/scale-to-zero-endpoint/llama3-8b-scale-to-zero-autoscaling.ipynb)\n",
    "- [💻 Deploy DeepSeek R1 Large Language Model from HuggingFace Hub on Amazon SageMaker](https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Deepseek/DeepSeek-R1-Llama8B-LMI-TGI-Deploy.ipynb)\n",
    "- [Available AWS Deep Learning Containers (DLC) images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
